{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "page_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boGw13u4B0ez"
      },
      "source": [
        "# Classificazione pagine wikipedia\n",
        "\n",
        "Codice per classificare le pagine di wikipedia nella versione inglese tramite l'utilizzo dei nodi wikidata associati alle pagine (laddove presenti ed utilizzabili) e il classificatore di pagine [ORES](https://www.mediawiki.org/wiki/ORES).\n",
        "Le pagine da classificare saranno quelle presenti nelle pagine più visitate nei mesi tra il 2016-2020 e saranno caricate su MongoDB in una collection a parte.\n",
        "\n",
        "Per ogni pagina sarà generato un documento strutturrato come il seguente (esempio David Bowie):\n",
        "``` python\n",
        "{\n",
        "    \"_id\":\"8786\", # page_id\n",
        "    \"article\":\"David_Bowie\", # page title\n",
        "    \"category\":\"arts, culture and entertainment\", # category with taxonomy ad hoc\n",
        "    \"description\":\"British singer, musician, and actor (1947-2016)\", # description \n",
        "    \"node_id\":\"Q5\", # node id type\n",
        "    \"ores_cat\":\"Culture.Media.Music\", # cateogry predicted by ores classifier\n",
        "    \"page_type\":\"biography\", # type of the page (other or biography)\n",
        "    \"rev_id\": 1015654584, # id last revison\n",
        "    \"wikidata_id\":\"Q5383\" # id page associated wikidata node\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9mqQO5JExQk"
      },
      "source": [
        "## Carico funzioni e librerie necessarie\n",
        "Alcune funzioni sono presenti nella repository github del progetto al seguente [link](https://github.com/riccardorubini98/wikitrend)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9yden7ZFM_l"
      },
      "source": [
        "pip install dnspython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nsvNWgqFS6b"
      },
      "source": [
        "import requests\n",
        "import time as time\n",
        "from tqdm.notebook import tqdm\n",
        "from requests.exceptions import ConnectionError\n",
        "# Import github repository\n",
        "!git clone https://github.com/riccardorubini98/wikitrend.git\n",
        "import sys\n",
        "sys.path.append(\"/content/wikitrend/\")\n",
        "import classifier_function as ct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlA3eGJaIFK0"
      },
      "source": [
        "# Collegamento a MongoDB\n",
        "Mi collego a MongoDB Atlas e scarico la collezione contente le pagine più visitate mese per mese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsaLXi8lIaK1"
      },
      "source": [
        "# Connect mongodb atlas\n",
        "from pymongo import MongoClient\n",
        "access_token = \"\"\n",
        "client = MongoClient(access_token)\n",
        "db = client.get_database('wikitrend')\n",
        "records = db.toppage.find() # top page for month\n",
        "pages = db.page_topic # new collection for single pages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m06eKSJI-g3"
      },
      "source": [
        "# Creazione loop per classificare le pagine\n",
        "Creo un ciclo for per ogni documento e per ogni pagina presente nella collezione contenente le pagine più viste per mese. Se la pagina presa in esame non è già stata classificata allora procedo alla categorizzazione, in caso contrario passo alla pagina successiva.\n",
        "\n",
        "L'algoritmo per la classificazione funziona in questo modo:\n",
        "1. Assegno gli ids necessari alla pagina per essere classificata. Fra questi può essere presente l'id del nodo wikidata associato.\n",
        "2. Se l'id wikidata è presente, cerco la tipologia del nodo associato alla pagina e in base a questa definisco la tipologia della pagina (biografica o altro) e in alcuni casi già la categoria ORES associata (ad esempio se il nodo è relativo ad un Film la categoria ORES sarà senz'altro *culture.media.films*)\n",
        "3. Per le pagina che con il punto precedente non sono state classificate utilizzo il classificatore ORES con il metodo *draft topic*. Qual ora questo non producesse risultati utili eseguo un nuovo tentativo con il metodo *article topic*.\n",
        "4. Converto la categoria ORES nella tassonomia utilizzata nel progetto. Le nuove cateogire sono: art, culture and entertainment; history and geography, internet and game, sport, STEM and other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97pVZFd-LQDR"
      },
      "source": [
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
        "\n",
        "for record in records[29:]:\n",
        "    start = time.perf_counter()\n",
        "    print(\"Analyze record: \", record[\"_id\"],\"\\n\")\n",
        "    top_month = record[\"top_month\"]\n",
        "    top_day = record[\"top_day\"]\n",
        "    \n",
        "    # select unique page from top_month and top_day list\n",
        "    id_list = []\n",
        "    page_list = []\n",
        "    for articolo in top_month:\n",
        "      id_list.append(articolo[\"page_id\"])\n",
        "      page_list.append(articolo)\n",
        "    for articolo in top_day:\n",
        "      page_id = articolo[\"page_id\"]\n",
        "      if page_id not in id_list:\n",
        "        page_list.append(articolo)\n",
        "\n",
        "    # find new pages to categorize and upload\n",
        "    new_pages = []\n",
        "    for articolo in tqdm(page_list,desc='Find new pages \\n'):\n",
        "        page_id = articolo[\"page_id\"]\n",
        "        # pages without page_id\n",
        "        if page_id == -1 or page_id == None:\n",
        "            pass\n",
        "        else:\n",
        "            # search in mongodb collection\n",
        "            find_page = pages.find_one({\"_id\": page_id})\n",
        "            # if there isn't any page with the same _id\n",
        "            if find_page == None:\n",
        "                new_pages.append(articolo)\n",
        "            else:\n",
        "                pass\n",
        "    \n",
        "    print(\"New page: \", len(new_pages), \"\\n\")  \n",
        "    \n",
        "    # categorizing new pages\n",
        "    # adding ids\n",
        "    s = requests.session()\n",
        "    s.headers.update(headers)\n",
        "    for articolo in tqdm(new_pages,desc='Adding ids \\n'):\n",
        "        url = ct.api_id(articolo[\"article\"])\n",
        "        # make api call\n",
        "        i=0\n",
        "        not_found=True\n",
        "        while i <30 and not_found:\n",
        "            try:\n",
        "                r = s.get(url=url,timeout=20)\n",
        "                not_found=False\n",
        "            except ConnectionError:\n",
        "                time.sleep(1)\n",
        "                i += 1\n",
        "        r= r.json()\n",
        "        # Adding id to page document\n",
        "        articolo[\"page_id\"] = ct.get_pageid(r)\n",
        "        articolo[\"rev_id\"] = ct.get_revid(r)\n",
        "        articolo[\"wikidata_id\"] = ct.get_wikidataid(r)\n",
        "    \n",
        "    # get node_type id\n",
        "    s = requests.session()\n",
        "    s.headers.update(headers)\n",
        "    for articolo in tqdm(new_pages,desc=\"Adding node type \\n\"):\n",
        "        if \"wikidata_id\" in articolo.keys() and articolo[\"wikidata_id\"] != None:\n",
        "            url = ct.api_nodeid(articolo[\"wikidata_id\"])\n",
        "            # make api call\n",
        "            i=0\n",
        "            not_found=True\n",
        "            while i <30 and not_found:\n",
        "                try:\n",
        "                    r = s.get(url=url,timeout=20)\n",
        "                    not_found=False\n",
        "                except ConnectionError:\n",
        "                    time.sleep(1)\n",
        "                    i += 1\n",
        "            r= r.json()\n",
        "            node_id = ct.get_nodeid(r)\n",
        "            articolo[\"node_id\"] = node_id\n",
        "            if node_id != None:\n",
        "                nodetype = ct.get_nodetype(node_id)\n",
        "                if nodetype == None:\n",
        "                    pass\n",
        "                else:\n",
        "                    if len(nodetype) == 1:\n",
        "                        articolo[\"page_type\"] = nodetype[0]\n",
        "                    elif len(nodetype) == 2:\n",
        "                        articolo[\"page_type\"] = nodetype[0]\n",
        "                        articolo[\"ores_cat\"] = nodetype[1]\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    # get_category\n",
        "    s = requests.session()\n",
        "    s.headers.update(headers)\n",
        "    for articolo in tqdm(new_pages, desc=\"Getting category \\n\"):\n",
        "        if \"ores_cat\" in articolo.keys():\n",
        "            articolo[\"category\"] = ct.ores_converter(pred_cat)\n",
        "        else:\n",
        "            url = ct.api_draftcat(articolo[\"rev_id\"])\n",
        "            i=0\n",
        "            not_found=True\n",
        "            while i <30 and not_found:\n",
        "                try:\n",
        "                    r = s.get(url=url)\n",
        "                    not_found=False\n",
        "                except ConnectionError:\n",
        "                    time.sleep(1)\n",
        "                    i += 1\n",
        "            r= r.json()\n",
        "            cat = ct.get_draftcat(r)\n",
        "            # If there is a cateogry of biographical type then the page type is \"biography\"\n",
        "            if 'Culture.Biography.Biography*' in cat or 'Culture.Biography.Women' in cat:\n",
        "                articolo[\"page_type\"] = \"biography\"            \n",
        "            draft_cat = ct.clean_cat(cat)\n",
        "            # If with draft method I don't get a useful category I try with article method\n",
        "            if draft_cat[:3] == \"Geo\" or draft_cat == \"other.category\":\n",
        "                url = ct.api_articlecat(articolo[\"rev_id\"])\n",
        "                i=0\n",
        "                not_found=True\n",
        "                while i <30 and not_found:\n",
        "                    try:\n",
        "                        r = s.get(url=url)\n",
        "                        not_found=False\n",
        "                    except ConnectionError:\n",
        "                        time.sleep(1)\n",
        "                        i += 1\n",
        "                r= r.json()\n",
        "                cat2 = ct.get_articlecat(r)      \n",
        "                article_cat = ct.clean_cat(cat2)\n",
        "                # select best category from draft and article method\n",
        "                pred_cat = ct.clean_cat([draft_cat,article_cat])\n",
        "            else:\n",
        "                pred_cat = draft_cat\n",
        "            # If a page hasn't page_type then it's \"other\"\n",
        "            if \"page_type\" not in articolo.keys():\n",
        "                articolo[\"page_type\"] = \"other\"\n",
        "            # If a page is biography and the predicted category is of geographical type then that page isn't classified\n",
        "            if articolo[\"page_type\"] == \"biography\" and pred_cat[:3] == \"Geo\":\n",
        "                    pred_cat = \"other.category\"\n",
        "            # Save category\n",
        "            articolo[\"ores_cat\"] = pred_cat\n",
        "            # Convert category\n",
        "            articolo[\"category\"] = ct.ores_converter(pred_cat)\n",
        "\n",
        "    # Upload new page on MongoDB\n",
        "    for page in tqdm(new_pages,desc= \"Uploading pages \\n\"):\n",
        "        page[\"_id\"] = page[\"page_id\"]\n",
        "        del page[\"page_id\"]\n",
        "        del page[\"views\"]\n",
        "        if \"day_of_month\" in page.keys():\n",
        "            del page[\"day_of_month\"]\n",
        "        try:\n",
        "            pages.insert_one(page)\n",
        "        except:\n",
        "            print(page)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}