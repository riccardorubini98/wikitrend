{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"info_pagine_parte1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HfysGaAQItt2"},"source":["In questa pagina viene presentato il codice python che si è reso necessario per portare a termine la prima parte del task, vale a dire l'ottenimento dei dati di interesse (per ogni pagina wiki), realizzazione dei rispettivi documenti json e la loro immissione sulla coda Kafka ad opera di un Kafka Producer. Sotto sono riportati i vari passi della procedura."]},{"cell_type":"markdown","metadata":{"id":"i3IRpsW9Nloh"},"source":["#Installazione delle librerie necessarie"]},{"cell_type":"code","metadata":{"id":"rTBecVwOtkOl"},"source":["pip install pymongo[srv]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtU_Ne6fIT0j"},"source":["pip install aiohttp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMm7CPBcIf2j"},"source":["pip install asyncio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NyD0fJVIl0D"},"source":["pip install async_retrying"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csN9lZJBInnQ"},"source":["pip install kafka-python"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AN3gFCHmSYTV"},"source":["# Collegamento a MongoDB"]},{"cell_type":"markdown","metadata":{"id":"96MNPEhEJJlx"},"source":["Nel chunk sottostante avviene  dapprima il collegamento alla piattaforma *mongodbatlas*; successivamente, si realizza l'accesso al database *wikitrend* ed infine alla collezione di interesse, *page_topic*, istanziata col nome *input*."]},{"cell_type":"code","metadata":{"id":"kTrHS22auAU_"},"source":["from pymongo import MongoClient\n","client = MongoClient(\"\")\n","# DB selection\n","db = client.get_database('wikitrend')\n","# Selection of input collection\n","input = db.page_topic"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zGksJeRATAQV"},"source":["# Presentazione di due funzioni utili"]},{"cell_type":"markdown","metadata":{"id":"PW1jbjWJLmwO"},"source":["Nei successivi due chunk vengono presentate due funzioni che risultano particolarmente utili per il compimento del task d'interesse."]},{"cell_type":"markdown","metadata":{"id":"p_1Y3I03HKp4"},"source":["La funzione sottostante riceve in input (sottoforma di stringa) il nome di una pagina wiki, e restituisce in output una lista di nove urls relativi ad altrettanti endpoints a cui si dovrà fare richiesta successivamente. Come già accennato, sette di questi sono relativi all'API di *wikimedia* (accessibile al seguente [link](https://wikimedia.org/api/rest_v1/#/)), mentre i restanti fanno riferimento all'API di *xtools* (accessibile al seguente [link](https://xtools.readthedocs.io/en/3.1.41/api/index.html))."]},{"cell_type":"code","metadata":{"id":"DrRq1g3BKl-s"},"source":["import urllib.parse\n","def get_urls(page_name):\n","  l=[]\n","  years = [('20150701','20151231'),('20160101','20161231'),('20170101','20171231'),('20180101','20181231'),\n","          ('20190101','20191231'),('20200101','20201231')]\n","  for year in years:\n","    url = 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/'+urllib.parse.quote(page_name, safe='')+'/all-editor-types/monthly/'+year[0]+'/'+year[1]\n","    l.append(url)\n","  \n","  views = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/'+urllib.parse.quote(page_name, safe='')+'/monthly/20150701/20201231'\n","  prose = 'https://xtools.wmflabs.org/api/page/prose/en.wikipedia.org/'+urllib.parse.quote(page_name, safe='')\n","  creation = 'https://xtools.wmflabs.org/api/page/articleinfo/en.wikipedia.org/'+urllib.parse.quote(page_name, safe='')\n","\n","  l.append(views)\n","  l.append(prose)\n","  l.append(creation)\n","  return l #ouptut: list of 9 urls given a page name"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvGPSX7vK3Zd"},"source":["La funzione sottostante (*get_dict*) riceve in input la lista di dizionari, li tratta opportunamente e restituisce in output il dizionario definitivo, contenente le info relative ad una specifica pagina wiki, il quale verrà poi immesso sulla coda Kafka da un Kafka Producer (vedi codice sotto). I dizionari di partenza sono assimilabili alle materie prime dalle quali, mediante un processo di lavorazione (la funzione *get_dict*), viene realizzato il prodotto finito (ovvero, il dizionario definitivo)."]},{"cell_type":"code","metadata":{"id":"nHDT78ZmFk7j"},"source":["from datetime import datetime\n","\n","def get_dict(data):\n","  n_views = 0\n","  n_edits = 0\n","  months_views = 0\n","  months_edits = 0\n","  diz = {}\n","  for d in data:\n","    if 'created_at' in d:\n","      date_of_creation = datetime.strptime(d['created_at'], '%Y-%m-%d') # catch the date of creation\n","  for d in data: \n","    if 'items' in d: \n","      if 'results' in d['items'][0]:\n","        for el in d['items'][0]['results']:\n","          date = datetime.strptime(el['timestamp'][:10], '%Y-%m-%d') \n","          if date > date_of_creation:\n","            n_edits += el['edits']\n","            months_edits += 1\n","            avg_edits = int(round(n_edits/months_edits))\n","            diz['mean_edits'] = avg_edits\n","      else:\n","        for el in d['items']:\n","          n_views += el['views']\n","          months_views += 1\n","          avg_views = int(round(n_views/months_views))\n","          diz['mean_views'] = avg_views\n","    elif 'words' in d:\n","      diz['words'] = d['words']\n","    elif 'created_at' in d: \n","      diz['y_of_creation'] = d['created_at'].split('-')[0]\n","    else:\n","      diz.update(d)\n","  return diz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cw__W8p-zsrP","executionInfo":{"status":"ok","timestamp":1624524123848,"user_tz":-120,"elapsed":1861,"user":{"displayName":"Alfredo Galli","photoUrl":"","userId":"09827124735713827283"}},"outputId":"8f7a867c-6a4c-4baa-89ce-6ae99296386e"},"source":["for doc in input.find({'article':'Al_Pacino'}):\n","  d = {}\n","  d['article'] = doc['article']\n","  d['_id'] = doc['_id']\n","  urls = get_urls(doc['article'])\n","  l=[]\n","  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n","  import requests\n","  for url in urls:\n","    try:\n","      r = requests.get(url, headers = headers)\n","      r.raise_for_status()\n","      data = r.json()\n","      l.append(data)\n","    except requests.exceptions.HTTPError as errh:\n","      print (\"Http Error:\",errh)\n","    except requests.exceptions.ConnectionError as errc:\n","      print (\"Error Connecting:\",errc)\n","    except requests.exceptions.Timeout as errt:\n","      print (\"Timeout Error:\",errt)\n","    except requests.exceptions.RequestException as err:\n","      print (\"OOps: Something Else\",err)\n","  l.append(d)\n","\n","diz = get_dict(l)\n","print(diz)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mean_edits': 21, 'mean_views': 323061, 'words': 4482, 'y_of_creation': '2002', 'article': 'Al_Pacino', '_id': 41906}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5gh7Ydab77-Z"},"source":["Sopra è riportato un esempio di dizionario restituito dalla funzione get_dict e riferito alla pagina wiki 'Al_Pacino', contenente le varie informazioni di interesse."]},{"cell_type":"markdown","metadata":{"id":"cGAt1-NxTiId"},"source":["# Inizializzazione Kafka Producer"]},{"cell_type":"markdown","metadata":{"id":"gJmv813aEhgQ"},"source":["Nel breve codice sottostante viene inizializzato un Kafka Producer, che successivamente sarà adibito alla scrittura dei dati sulla coda Kafka. Il dato in un input viene convertito in un file *json* e codificato in utf-8."]},{"cell_type":"code","metadata":{"id":"AEL6uFcTEWlz"},"source":["producer = KafkaProducer(bootstrap_servers=['kafka:9092'], # This piece of code initializes a new Kafka Producer\n","                         value_serializer=lambda x: \n","                         dumps(x).encode('utf-8'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVzdurh7TrEM"},"source":["# Download dei dati di interesse e scrittura su Kafka"]},{"cell_type":"markdown","metadata":{"id":"C1efYjACIZfw"},"source":["Nel codice sotto riportato viene realizzata una lista di liste che prende il nome di *tot_urls*. Ciascun elemento (lista) è associato ad una pagina wiki e contiene l'elenco dei nove urls in aggiunta al dizionario contenente il nome dell'articolo e il relativo id nella collezione mongo *page_topic*. Pertanto, la lunghezza della lista di liste è pari al numero di pagine. Un elemento della lista (riferito alla pagina wiki 'David_Bowie') è stampato in coda al codice."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hb22co2_GRXZ","executionInfo":{"status":"ok","timestamp":1624523222638,"user_tz":-120,"elapsed":857,"user":{"displayName":"Alfredo Galli","photoUrl":"","userId":"09827124735713827283"}},"outputId":"8075f1d3-5d30-461e-b802-6f5edc72e861"},"source":["tot_urls = []\n","for doc in input.find():\n","  d = {}\n","  d['article'] = doc['article']\n","  d['_id'] = doc['_id'] # the dict d contains informations about the name of the wiki page and its _id in the input collection 'page_topic'\n","  urls = get_urls(doc['article'])# the list of urls is created\n","  urls.append(d) #little trick: append d to the list of urls. This will turn to be useful later\n","  tot_urls.append(urls)\n","print(tot_urls[0])\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/David_Bowie/all-editor-types/monthly/20150701/20151231', 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/David_Bowie/all-editor-types/monthly/20160101/20161231', 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/David_Bowie/all-editor-types/monthly/20170101/20171231', 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/David_Bowie/all-editor-types/monthly/20180101/20181231', 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/David_Bowie/all-editor-types/monthly/20190101/20191231', 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/David_Bowie/all-editor-types/monthly/20200101/20201231', 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/David_Bowie/monthly/20150701/20201231', 'https://xtools.wmflabs.org/api/page/prose/en.wikipedia.org/David_Bowie', 'https://xtools.wmflabs.org/api/page/articleinfo/en.wikipedia.org/David_Bowie', {'article': 'David_Bowie', '_id': 8786}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UQfsfQzSOvYj"},"source":["Ora, il chunk sotto riportato costituisce il \"cuore\" del processo: la composizione delle tre funzioni *async* consente di effettuare le chiamate agli endpoints contenuti nella lista *tot_urls* in modalità completamente asincrona. Detto in brevi termini, ciò significa che le richieste (tasks) ai vari endpoints vengono effettuati simultaneamente e in maniera pressochè indipendente. In questo modo viene garantita una estrema efficienza riducendo notevolmente i tempi dell'operazione (si tenga conto che, in presenza di 9235 pagine di wiki, il totale delle richieste da effettuare ammonta a 92535 X 9 = 83115 urls). Si osserva che nella seconda funzione (innestata poi nella terza), una volta che i tasks relativi ad una pagina wiki sono completati, il producer si adopera nella scrittura del dizionario restituito dalla funzione get_dict sulla coda Kafka. Tale struttura dati, come visto in precedenza, viene dapprima convertita in un file json codificato in utf-8, il quale successivamente viene immesso nel topic *trial*."]},{"cell_type":"code","metadata":{"id":"62u2fe4yOrx8"},"source":["import asyncio\n","import aiohttp\n","from async_retrying import retry\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","\n","# The composition of these three functions allows us to make a parallel request to the urls gained by the get_url function,\n","# in order to save a lot of time during the extracting data process\n","\n","@retry(attempts=5000) #number of attempts to get the result from each endpoint, high number to be conservative...\n","async def get(url, session):\n","  async with session.get(url) as resp:\n","      resp.raise_for_status()\n","      return await resp.json() #This returns a json object of the result, basically a Python dictionary\n","\n","async def fetch(urls, producer):\n","  async with aiohttp.ClientSession() as session:\n","      tasks = [get(url, session) for url in urls[:-1]]\n","      resp = await asyncio.gather(*tasks)\n","      resp.append(urls[-1])\n","      d = get_dict(resp) #once the tasks of a wiki page are completed, the dict is created through the function 'get_dict'\n","      producer.send('trial', value = d)  #producer sends the data to the topic trial\n","\n","async def main(tot_urls):\n","  cors = [fetch(urls) for urls in tot_urls]\n","  await asyncio.gather(*cors)\n","\n","if __name__ == \"__main__\":\n","  asyncio.run(main(tot_urls))"],"execution_count":null,"outputs":[]}]}